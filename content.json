{"meta":{"title":"Блог о программировании и аналитике","subtitle":"и тут что-то","description":null,"author":"Confar","url":"http://confar.github.io/blog"},"pages":[{"title":"Обо мне","date":"2017-10-11T09:15:19.000Z","updated":"2017-10-17T09:10:58.158Z","comments":true,"path":"about/index.html","permalink":"http://confar.github.io/blog/about/index.html","excerpt":"","text":"@ card {In Page page, we recommend that the contents are placed in Card in.Александр}"},{"title":"Теги","date":"2017-10-07T22:16:30.000Z","updated":"2017-10-16T07:29:08.795Z","comments":false,"path":"tags/index.html","permalink":"http://confar.github.io/blog/tags/index.html","excerpt":"","text":""},{"title":"categories","date":"2017-10-07T22:16:49.000Z","updated":"2017-10-07T22:17:01.640Z","comments":false,"path":"categories/index.html","permalink":"http://confar.github.io/blog/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"Пишем таск менеджер на Django","slug":"Разбираемся-c-Django","date":"2017-10-16T12:13:04.000Z","updated":"2017-10-17T10:56:47.358Z","comments":true,"path":"2017/10/16/Разбираемся-c-Django/","link":"","permalink":"http://confar.github.io/blog/2017/10/16/Разбираемся-c-Django/","excerpt":"","text":"Пишем таск менеджер на Django Начнем со страницы авторизации.","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://confar.github.io/blog/tags/python/"},{"name":"Django","slug":"Django","permalink":"http://confar.github.io/blog/tags/Django/"},{"name":"web","slug":"web","permalink":"http://confar.github.io/blog/tags/web/"}]},{"title":"Web Scraping","slug":"web-scraping","date":"2017-10-16T11:35:12.000Z","updated":"2017-10-17T09:44:27.926Z","comments":true,"path":"2017/10/16/web-scraping/","link":"","permalink":"http://confar.github.io/blog/2017/10/16/web-scraping/","excerpt":"","text":"Сегодня поговорим о веб скрапинге. Допустим, стоит задача выгрузить информацию с сайта или нескольких и представить ее в таблице. Для примера я возьму techcrunch и mashable. На выходе хочу получить таблицу с колонками [текст], [краткое описание], [ссылка], чтобы не тратить время на бесконечное зависание на сайтах, ну вы знаете как это бывает=) БиблиотекиНачнем с импорта. Для работы с http лучший выбор это requests, а для парсинга html будем классически работать с Beautiful Soup. Ну и pandas для удобной работы с данными и выгрузки в csv. 1234import requestsfrom bs4 import BeautifulSoupfrom numpy import nan as NaNimport pandas as pd Нахождение нужных нам элементовНапишем скрипт, выполняющий get-запрос, записав его в переменную, затем преобразуем полученный текст страницы в объект BeautifulSoup.В качестве аргумента get-запроса передадим ссылку на раздел startups, там все веселье. 12r1 = requests.get('https://techcrunch.com/startups/')soup = BeautifulSoup(r1.text, 'html.parser') Здесь мы получим на выходе кусок html, который выглядит примерно так(для примера приведен только отрывок). 1234567&lt;ul class=\"subnav-channel\" data-omni-sm-delegate=\"gbl_mainnav\"&gt;&lt;li class=\"menu-item menu-item-type-taxonomy menu-item-object-category current-menu-item menu-item-899745\" id=\"menu-item-899745\"&gt;&lt;a href=\"https://techcrunch.com/startups/\"&gt;Startups&lt;/a&gt;&lt;/li&gt;&lt;li class=\"menu-item menu-item-type-taxonomy menu-item-object-category current-post-ancestor current-menu-parent current-post-parent menu-item-899746\" id=\"menu-item-899746\"&gt;&lt;a href=\"https://techcrunch.com/mobile/\"&gt;Mobile&lt;/a&gt;&lt;/li&gt;&lt;li class=\"menu-item menu-item-type-taxonomy menu-item-object-category menu-item-899747\" id=\"menu-item-899747\"&gt;&lt;a href=\"https://techcrunch.com/gadgets/\"&gt;Gadgets&lt;/a&gt;&lt;/li&gt;&lt;li class=\"menu-item menu-item-type-taxonomy menu-item-object-category menu-item-899748\" id=\"menu-item-899748\"&gt;&lt;a href=\"https://techcrunch.com/enterprise/\"&gt;Enterprise&lt;/a&gt;&lt;/li&gt;&lt;li class=\"menu-item menu-item-type-taxonomy menu-item-object-category menu-item-899749\" id=\"menu-item-899749\"&gt;&lt;a href=\"https://techcrunch.com/social/\"&gt;Social&lt;/a&gt;&lt;/li&gt;&lt;li class=\"menu-item menu-item-type-taxonomy menu-item-object-category menu-item-899750\" id=\"menu-item-899750\"&gt;&lt;a href=\"https://techcrunch.com/europe/\"&gt;Europe&lt;/a&gt;&lt;/li&gt;&lt;li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-901944\" id=\"menu-item-901944\"&gt;&lt;a href=\"/asia\"&gt;Asia&lt;/a&gt;&lt;/li&gt; Открываем нашу ссылку в браузере через F12 и находим нужные нам элементы - тег с названием(будем искать по title), описанием(description) и url. В случае с techcrunch все нужные нам ссылки находятся в “диве” block-content, в котором находится каждый пост. Для подобного сценария в BS есть команда find_all(), которая находит все возможные вхождения тега с заданными параметрами.Затем мы создаем простой for луп и пустой список, куда записываем нужные нам данные, которые также ищем по тегам внутри block-content. 1234567resultstech = soup.find_all('div', 'block-content',)recordstech = []for result in resultstech: title = result.find('h2').text.strip() desc = result.find('p', 'excerpt').text[:-10].strip() link = result.find('a')['href'] recordstech.append((title, desc, link)) Для удобства обрезаем пробелы с помощью метода strip(), и последние десять символов в desc, обратившись к ним по индексу[:-10], т.к. они представляют собой фразу “Read More” для каждого поста. На выходе мы получили список из кортежей, где находятся наши данные. Проделаем тоже самое с mashable123456789r2 = requests.get('http://mashable.com/tech/')soup = BeautifulSoup(r2.text, 'html.parser')resultsmash = soup.find_all('article')recordsmash = []for result in resultsmash: title = result.find('h1').text link = result.find('a')['href'] recordsmash.append((title, link)) Тут не всегда есть подробные описания, поэтому мы остановимся на названии и ссылке. Обработка с помощью pandasСоздадим два датафрейма pandas для того, чтобы красиво соединить наши данные. Для визуального разделения данных сделаем пустой ряд - empty row с помощью вставки not a number - “NaN” из numpy, после каждого набора данных. 1234567dftech = pd.DataFrame(recordstech, columns=['title', 'desc', 'link'])empty_row3 = pd.Series([NaN, NaN, NaN], index=['title', 'desc', 'link'])dftech_empty_row = dftech.append(empty_row3, ignore_index=True)dfmash = pd.DataFrame(recordsmash, columns=['title', 'link'])empty_row2 = pd.Series([NaN, NaN], index=['title', 'link'])dfmash_empty_row = dfmash.append(empty_row2, ignore_index=True) После этого соединим наши датафреймы с помощью метода concat() 12frames = [dftech_empty_row, dfmash_empty_row]results = pd.concat(frames) Открываем таблицуЗапишем результаты в csv 1results.to_csv('parsing.csv'), index=False, encoding='utf-8') откроем csv файл с помощью текстового редактора и зададим разделение sep=, в самом начале если нужно открыть в экселе либо откроем в google sheets Картинка ниже как именно выглядит результат Готово. Можете применить подобную операцию ко всем сайтам в интересной вам индустрии, и на выходе получить сводку всех новостей в одном таблице, определив что вам полезно, а что нет. И не тратить время на бесконечный скроллинг веба=) Предварительно ознакомьтесь с правилами сайта, для этого прибавьте к корневому url адресу /robots.txt и узнайте, можно ли его “скрейпить”. Некоторые сайты предоставляют отдельное api для скрейпинга, но об этом уже в другой раз. Ссылки Полный код можно найти в моем репозитории на github Документация BeautifulSoup Классная статья о скрейпинге общественных ресурсов","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://confar.github.io/blog/tags/python/"},{"name":"scraping","slug":"scraping","permalink":"http://confar.github.io/blog/tags/scraping/"},{"name":"pandas","slug":"pandas","permalink":"http://confar.github.io/blog/tags/pandas/"}]},{"title":"Hello World","slug":"hello-world","date":"2017-10-07T18:23:17.467Z","updated":"2017-10-17T09:17:06.736Z","comments":true,"path":"2017/10/07/hello-world/","link":"","permalink":"http://confar.github.io/blog/2017/10/07/hello-world/","excerpt":"","text":"Решил поднять сайт. Тут будут разные интересности про программирование и аналитику. 1import this","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://confar.github.io/blog/tags/python/"}]}]}