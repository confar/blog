<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Блог о программировании и аналитике</title>
  
  <link href="/blog/atom.xml" rel="self"/>
  
  <link href="http://confar.github.io/blog/"/>
  <updated>2017-10-17T09:15:34.540Z</updated>
  <id>http://confar.github.io/blog/</id>
  
  <author>
    <name>Confar</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Пишем таск менеджер на Django</title>
    <link href="http://confar.github.io/blog/2017/10/16/%D0%A0%D0%B0%D0%B7%D0%B1%D0%B8%D1%80%D0%B0%D0%B5%D0%BC%D1%81%D1%8F-c-Django/"/>
    <id>http://confar.github.io/blog/2017/10/16/Разбираемся-c-Django/</id>
    <published>2017-10-16T12:13:04.000Z</published>
    <updated>2017-10-17T09:15:34.540Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Пишем-таск-менеджер-на-Django"><a href="#Пишем-таск-менеджер-на-Django" class="headerlink" title="Пишем таск менеджер на Django"></a>Пишем таск менеджер на Django</h1><p>  Начнем со страницы авторизации</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Пишем-таск-менеджер-на-Django&quot;&gt;&lt;a href=&quot;#Пишем-таск-менеджер-на-Django&quot; class=&quot;headerlink&quot; title=&quot;Пишем таск менеджер на Django&quot;&gt;&lt;/a
    
    </summary>
    
    
      <category term="python" scheme="http://confar.github.io/blog/tags/python/"/>
    
      <category term="Django" scheme="http://confar.github.io/blog/tags/Django/"/>
    
      <category term="web" scheme="http://confar.github.io/blog/tags/web/"/>
    
  </entry>
  
  <entry>
    <title>Web Scraping</title>
    <link href="http://confar.github.io/blog/2017/10/16/web-scraping/"/>
    <id>http://confar.github.io/blog/2017/10/16/web-scraping/</id>
    <published>2017-10-16T11:35:12.000Z</published>
    <updated>2017-10-17T08:59:09.660Z</updated>
    
    <content type="html"><![CDATA[<p>Сегодня поговорим о веб скрапинге. Допустим, стоит задача выгрузить информацию с сайта или нескольких и представить ее в таблице. Для примера я возьму techcrunch и mashable. На выходе хочу получить таблицу с колонками [текст], [краткое описание], [ссылка], чтобы не тратить время на бесконечное зависание на сайтах, ну вы знаете как это бывает=)</p>
<h1 id="Библиотеки"><a href="#Библиотеки" class="headerlink" title="Библиотеки"></a>Библиотеки</h1><p>Начнем с импорта. Для работы с http лучший выбор это requests, а для парсинга html будем классически работать с Beautiful Soup. Ну и pandas для удобной работы с данными и выгрузки в csv.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> requests</div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> nan <span class="keyword">as</span> NaN</div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div></pre></td></tr></table></figure>
<h1 id="Нахождение-нужных-нам-элементов"><a href="#Нахождение-нужных-нам-элементов" class="headerlink" title="Нахождение нужных нам элементов"></a>Нахождение нужных нам элементов</h1><p>Напишем скрипт, выполняющий get-запрос, записав его в переменную, затем преобразуем полученный текст страницы в объект BeautifulSoup.<br>В качестве аргумента get-запроса передадим ссылку на раздел startups, там все веселье.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">r1 = requests.get(<span class="string">'https://techcrunch.com/startups/'</span>)</div><div class="line">soup = BeautifulSoup(r1.text, <span class="string">'html.parser'</span>)</div></pre></td></tr></table></figure>
<p>Здесь мы получим на выходе кусок html, который выглядит примерно так(для примера приведен только отрывок). Действительно суп=)</p>
<pre><code>&lt;ul class=&quot;subnav-channel&quot; data-omni-sm-delegate=&quot;gbl_mainnav&quot;&gt;&lt;li class=&quot;menu-item menu-item-type-taxonomy menu-item-object-category current-menu-item menu-item-899745&quot; id=&quot;menu-item-899745&quot;&gt;&lt;a href=&quot;https://techcrunch.com/startups/&quot;&gt;Startups&lt;/a&gt;&lt;/li&gt;
&lt;li class=&quot;menu-item menu-item-type-taxonomy menu-item-object-category current-post-ancestor current-menu-parent current-post-parent menu-item-899746&quot; id=&quot;menu-item-899746&quot;&gt;&lt;a href=&quot;https://techcrunch.com/mobile/&quot;&gt;Mobile&lt;/a&gt;&lt;/li&gt;
&lt;li class=&quot;menu-item menu-item-type-taxonomy menu-item-object-category menu-item-899747&quot; id=&quot;menu-item-899747&quot;&gt;&lt;a href=&quot;https://techcrunch.com/gadgets/&quot;&gt;Gadgets&lt;/a&gt;&lt;/li&gt;
&lt;li class=&quot;menu-item menu-item-type-taxonomy menu-item-object-category menu-item-899748&quot; id=&quot;menu-item-899748&quot;&gt;&lt;a href=&quot;https://techcrunch.com/enterprise/&quot;&gt;Enterprise&lt;/a&gt;&lt;/li&gt;
&lt;li class=&quot;menu-item menu-item-type-taxonomy menu-item-object-category menu-item-899749&quot; id=&quot;menu-item-899749&quot;&gt;&lt;a href=&quot;https://techcrunch.com/social/&quot;&gt;Social&lt;/a&gt;&lt;/li&gt;
&lt;li class=&quot;menu-item menu-item-type-taxonomy menu-item-object-category menu-item-899750&quot; id=&quot;menu-item-899750&quot;&gt;&lt;a href=&quot;https://techcrunch.com/europe/&quot;&gt;Europe&lt;/a&gt;&lt;/li&gt;
&lt;li class=&quot;menu-item menu-item-type-custom menu-item-object-custom menu-item-901944&quot; id=&quot;menu-item-901944&quot;&gt;&lt;a href=&quot;/asia&quot;&gt;Asia&lt;/a&gt;&lt;/li&gt;
</code></pre><p>Открываем нашу ссылку в браузере через f12 и находим нужные нам элементы - тег с названием(будем искать по title), описанием(description) и url.</p>
<p>В случае с techcrunch все нужные нам ссылки находятся в “диве” block-content, в котором находится каждый пост.</p>
<img src="/blog/2017/10/16/web-scraping/block-content.png" alt="block-content" title="block-content">
<p>Для подобного сценария в BS есть команда find<em>all(), которая находит </em>все_ возможные вхождения тега с заданными параметрами.<br>Затем мы создаем простой for луп и пустой список, куда записываем нужные нам данные, которые также ищем по тегам внутри block-content.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">resultstech = soup.find_all(<span class="string">'div'</span>, <span class="string">'block-content'</span>,)</div><div class="line">recordstech = []</div><div class="line"><span class="keyword">for</span> result <span class="keyword">in</span> resultstech:</div><div class="line">    title = result.find(<span class="string">'h2'</span>).text.strip()</div><div class="line">    desc = result.find(<span class="string">'p'</span>, <span class="string">'excerpt'</span>).text[:<span class="number">-10</span>].strip()</div><div class="line">    link = result.find(<span class="string">'a'</span>)[<span class="string">'href'</span>]</div><div class="line">    recordstech.append((title, desc, link))</div></pre></td></tr></table></figure>
<p>Для удобства обрезаем пробелы с помощью метода strip(), и последние десять символов в desc, обратившись к ним по индексу[:-10], т.к. они представляют собой фразу “Read More” для каждого поста.</p>
<p>На выходе мы получили список из кортежей, где находятся наши данные.</p>
<h2 id="Проделаем-тоже-самое-с-mashable"><a href="#Проделаем-тоже-самое-с-mashable" class="headerlink" title="Проделаем тоже самое с mashable"></a>Проделаем тоже самое с mashable</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">r2 = requests.get(<span class="string">'http://mashable.com/tech/'</span>)</div><div class="line">soup = BeautifulSoup(r2.text, <span class="string">'html.parser'</span>)</div><div class="line"></div><div class="line">resultsmash = soup.find_all(<span class="string">'article'</span>)</div><div class="line">recordsmash = []</div><div class="line"><span class="keyword">for</span> result <span class="keyword">in</span> resultsmash:</div><div class="line">    title = result.find(<span class="string">'h1'</span>).text</div><div class="line">    link = result.find(<span class="string">'a'</span>)[<span class="string">'href'</span>]</div><div class="line">    recordsmash.append((title, link))</div></pre></td></tr></table></figure>
<p>Тут не всегда есть подробные описания, поэтому мы остановимся на названии и ссылке.</p>
<h1 id="Обработка-с-помощью-pandas"><a href="#Обработка-с-помощью-pandas" class="headerlink" title="Обработка с помощью pandas"></a>Обработка с помощью pandas</h1><p>Создадим два датафрейма пандас для того, чтобы красиво соединить наши данные. Для визуального разделения данных сделаем пустой ряд - empty row с помощью вставки not a number - “NaN” из numpy, после каждого набора данных.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">dftech = pd.DataFrame(recordstech, columns=[<span class="string">'title'</span>, <span class="string">'desc'</span>, <span class="string">'link'</span>])</div><div class="line">empty_row3 = pd.Series([NaN, NaN, NaN], index=[<span class="string">'title'</span>, <span class="string">'desc'</span>, <span class="string">'link'</span>])</div><div class="line">dftech_empty_row = dftech.append(empty_row3, ignore_index=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">dfmash = pd.DataFrame(recordsmash, columns=[<span class="string">'title'</span>, <span class="string">'link'</span>])</div><div class="line">empty_row2 = pd.Series([NaN, NaN], index=[<span class="string">'title'</span>, <span class="string">'link'</span>])</div><div class="line">dfmash_empty_row = dfmash.append(empty_row2, ignore_index=<span class="keyword">True</span>)</div></pre></td></tr></table></figure>
<p>После этого соединим наши датафреймы с помощью метода concat()</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">frames = [dfvc_empty_row, dfmash_empty_row, dftech_empty_row]</div><div class="line">results = pd.concat(frames)</div></pre></td></tr></table></figure>
<h1 id="Открываем-таблицу"><a href="#Открываем-таблицу" class="headerlink" title="Открываем таблицу"></a>Открываем таблицу</h1><p>Запишем результаты в csv</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">results.to_csv(<span class="string">'parsing.csv'</span>), index=<span class="keyword">False</span>, encoding=<span class="string">'utf-8'</span>)</div></pre></td></tr></table></figure>
<ul>
<li>откроем csv файл с помощью текстового редактора и зададим разделение sep=, в самом начале  если нужно открыть в экселе</li>
<li>либо откроем в google sheets</li>
</ul>
<p>Картинка ниже как именно выглядит результат</p>
<img src="/blog/2017/10/16/web-scraping/result.png" alt="result" title="result">
<p>Готово. Можете применить подобную операцию ко всем сайтам в интересной вам индустрии, и на выходе получить сводку всех новостей в одном таблице, определив что вам полезно, а что нет. И не тратить время на бесконечный скроллинг веба=)</p>
<blockquote>
<p>Предварительно ознакомьтесь с правилами сайта, для этого прибавьте к корневому url адресу /robots.txt и узнайте, можно ли его “скрейпить”.</p>
</blockquote>
<p>Некоторые сайты предоставляют отдельное api для скрейпинга, но об этом уже в другой раз.</p>
<h1 id="Ссылки"><a href="#Ссылки" class="headerlink" title="Ссылки"></a>Ссылки</h1><ul>
<li><a href="https://github.com/confar/web-parsing-tech-news" target="_blank" rel="external">Полный код можно найти в моем репозитории на github</a></li>
<li><a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/" target="_blank" rel="external">Документация BeautifulSoup</a></li>
<li><a href="http://pbpython.com/web-scraping-mn-budget.html" target="_blank" rel="external">Классная статья о скрейпинге общественных ресурсов</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Сегодня поговорим о веб скрапинге. Допустим, стоит задача выгрузить информацию с сайта или нескольких и представить ее в таблице. Для при
    
    </summary>
    
    
      <category term="python" scheme="http://confar.github.io/blog/tags/python/"/>
    
      <category term="scraping" scheme="http://confar.github.io/blog/tags/scraping/"/>
    
      <category term="pandas" scheme="http://confar.github.io/blog/tags/pandas/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://confar.github.io/blog/2017/10/07/hello-world/"/>
    <id>http://confar.github.io/blog/2017/10/07/hello-world/</id>
    <published>2017-10-07T18:23:17.467Z</published>
    <updated>2017-10-17T09:17:06.736Z</updated>
    
    <content type="html"><![CDATA[<p>Решил поднять сайт. Тут будут разные интересности про программирование и аналитику.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> this</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Решил поднять сайт. Тут будут разные интересности про программирование и аналитику.&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td c
    
    </summary>
    
    
      <category term="python" scheme="http://confar.github.io/blog/tags/python/"/>
    
  </entry>
  
</feed>
